{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Dependencies",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "## Function to preprocess data for each sport",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This function handles preprocessing of each sport's dataset:\n\n    Fills missing values.\n    Encodes categorical features like Team 1 and Team 2 or Player 1 and Player 2.\n    Creates a binary target variable (match_result) for win/loss based on Score 1 and Score 2.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def preprocess_data(df, sport):\n    # Handle missing values\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n    \n    # Ensure 'Score 1' and 'Score 2' are numeric\n    if 'Score 1' in df.columns and 'Score 2' in df.columns:\n        df['Score 1'] = pd.to_numeric(df['Score 1'], errors='coerce')\n        df['Score 2'] = pd.to_numeric(df['Score 2'], errors='coerce')\n    \n    # Convert categorical columns to numerical (if applicable)\n    if 'Team 1' in df.columns and 'Team 2' in df.columns:\n        label_encoder = LabelEncoder()\n        df['Team 1'] = label_encoder.fit_transform(df['Team 1'])\n        df['Team 2'] = label_encoder.fit_transform(df['Team 2'])\n    \n    if 'Player 1' in df.columns and 'Player 2' in df.columns:\n        label_encoder = LabelEncoder()\n        df['Player 1'] = label_encoder.fit_transform(df['Player 1'])\n        df['Player 2'] = label_encoder.fit_transform(df['Player 2'])\n\n    # Create a new target column for binary classification (win/loss)\n    if 'Score 1' in df.columns and 'Score 2' in df.columns:\n        df['match_result'] = np.where(df['Score 1'] > df['Score 2'], 1, 0)\n        df.drop(['Score 1', 'Score 2'], axis=1, inplace=True)\n\n    return df\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "## Function to train models and compare results",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This function trains the models (XGBoost, Random Forest, and Logistic Regression) on the preprocessed data and evaluates each model's performance:\n\n    XGBoost is tuned using GridSearchCV.\n    Random Forest and Logistic Regression are trained with default settings.\n    For each model, accuracy and classification reports are printed.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def train_and_compare(df, sport):\n    print(f\"\\n--- Training Models for {sport} ---\")\n    \n    # Define features and target\n    if 'Team 1' in df.columns:\n        X = df[['Team 1', 'Team 2']]\n    else:\n        X = df[['Player 1', 'Player 2']]\n    \n    y = df['match_result']\n    \n    # Train-Test Split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Check the distribution of classes in y_train and y_test\n    print(f\"Class distribution in y_train for {sport}:\")\n    print(y_train.value_counts())\n    \n    print(f\"Class distribution in y_test for {sport}:\")\n    print(y_test.value_counts())\n    \n    # Proceed with the models only if there are at least two classes in y_train\n    if len(y_train.unique()) < 2:\n        print(f\"Skipping {sport} due to only one class in y_train.\")\n        return None  # Skip this sport\n        \n    # Dictionary to store results\n    results = {}\n    \n    # XGBoost Model with Hyperparameter Tuning\n    xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n    param_grid = {\n        'max_depth': [3, 7, 9],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'n_estimators': [100, 200, 300],\n        'colsample_bytree': [0.3, 0.7]\n    }\n    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\n    grid_search.fit(X_train, y_train)\n    \n    # Best model from Grid Search\n    best_xgb_model = grid_search.best_estimator_\n    \n    # Make Predictions\n    y_pred_xgb = best_xgb_model.predict(X_test)\n    \n    # Evaluate XGBoost Model\n    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n    print(f\"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%\")\n    results['XGBoost'] = xgb_accuracy\n    \n    # Random Forest Model\n    rf_model = RandomForestClassifier(random_state=42)\n    rf_model.fit(X_train, y_train)\n    y_pred_rf = rf_model.predict(X_test)\n    \n    # Evaluate Random Forest Model\n    rf_accuracy = accuracy_score(y_test, y_pred_rf)\n    print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n    results['Random Forest'] = rf_accuracy\n    \n    # Logistic Regression Model\n    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n    lr_model.fit(X_train, y_train)\n    y_pred_lr = lr_model.predict(X_test)\n    \n    # Evaluate Logistic Regression Model\n    lr_accuracy = accuracy_score(y_test, y_pred_lr)\n    print(f\"Logistic Regression Accuracy: {lr_accuracy * 100:.2f}%\")\n    results['Logistic Regression'] = lr_accuracy\n    \n    # Classification Report for each model\n    print(\"XGBoost Classification Report:\")\n    print(classification_report(y_test, y_pred_xgb))\n    \n    print(\"Random Forest Classification Report:\")\n    print(classification_report(y_test, y_pred_rf))\n    \n    print(\"Logistic Regression Classification Report:\")\n    print(classification_report(y_test, y_pred_lr))\n    \n    return results\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "## Loading Datasets and Applying Models\n\nHere we loop through the datasets for all 12 sports, apply the preprocessing, and train the models for each sport. We store the results for later comparison.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# List of sports datasets (replace with actual file paths)\nsports_datasets = {\n    'Basketball': 'basketball.csv',\n    'Cricket': 'cricket.csv',\n    'Hockey': 'hockey.csv',\n    'Handball': 'handball.csv',\n    'Ice Hockey': 'ice_hockey.csv',\n    'Lacrosse': 'lacrosse.csv',\n    'Roller Hockey': 'roller_hockey.csv',\n    'Rugby': 'rugby.csv',\n    'Soccer': 'soccer.csv',\n    'Volleyball': 'volleyball.csv',\n    'Water Polo': 'water_polo.csv'\n}\n\n# Dictionary to store results for all sports\nall_sports_results = {}\n\n# Loop through each sport, preprocess data, and train models\nfor sport, file in sports_datasets.items():\n    df = pd.read_csv(file)\n    df = preprocess_data(df, sport)\n    results = train_and_compare(df, sport)\n    all_sports_results[sport] = results\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n--- Training Models for Basketball ---\nClass distribution in y_train for Basketball:\nmatch_result\n1    322\n0    246\nName: count, dtype: int64\nClass distribution in y_test for Basketball:\nmatch_result\n1    72\n0    70\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 73.94%\nRandom Forest Accuracy: 72.54%\nLogistic Regression Accuracy: 58.45%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.75      0.71      0.73        70\n           1       0.73      0.76      0.75        72\n\n    accuracy                           0.74       142\n   macro avg       0.74      0.74      0.74       142\nweighted avg       0.74      0.74      0.74       142\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.74      0.69      0.71        70\n           1       0.71      0.76      0.74        72\n\n    accuracy                           0.73       142\n   macro avg       0.73      0.72      0.72       142\nweighted avg       0.73      0.73      0.72       142\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.62      0.41      0.50        70\n           1       0.57      0.75      0.65        72\n\n    accuracy                           0.58       142\n   macro avg       0.59      0.58      0.57       142\nweighted avg       0.59      0.58      0.57       142\n\n\n--- Training Models for Cricket ---\nClass distribution in y_train for Cricket:\nmatch_result\n1    190\n0    189\nName: count, dtype: int64\nClass distribution in y_test for Cricket:\nmatch_result\n1    54\n0    41\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 51.58%\nRandom Forest Accuracy: 48.42%\nLogistic Regression Accuracy: 64.21%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.43      0.37      0.39        41\n           1       0.57      0.63      0.60        54\n\n    accuracy                           0.52        95\n   macro avg       0.50      0.50      0.50        95\nweighted avg       0.51      0.52      0.51        95\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.40      0.39      0.40        41\n           1       0.55      0.56      0.55        54\n\n    accuracy                           0.48        95\n   macro avg       0.47      0.47      0.47        95\nweighted avg       0.48      0.48      0.48        95\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.59      0.59      0.59        41\n           1       0.69      0.69      0.69        54\n\n    accuracy                           0.64        95\n   macro avg       0.64      0.64      0.64        95\nweighted avg       0.64      0.64      0.64        95\n\n\n--- Training Models for Hockey ---\nClass distribution in y_train for Hockey:\nmatch_result\n1    291\n0    225\nName: count, dtype: int64\nClass distribution in y_test for Hockey:\nmatch_result\n1    79\n0    50\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 72.87%\nRandom Forest Accuracy: 58.14%\nLogistic Regression Accuracy: 61.24%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.71      0.50      0.59        50\n           1       0.73      0.87      0.80        79\n\n    accuracy                           0.73       129\n   macro avg       0.72      0.69      0.69       129\nweighted avg       0.73      0.73      0.72       129\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.46      0.52      0.49        50\n           1       0.67      0.62      0.64        79\n\n    accuracy                           0.58       129\n   macro avg       0.57      0.57      0.57       129\nweighted avg       0.59      0.58      0.58       129\n\nLogistic Regression Classification Report:\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        50\n           1       0.61      1.00      0.76        79\n\n    accuracy                           0.61       129\n   macro avg       0.31      0.50      0.38       129\nweighted avg       0.38      0.61      0.47       129\n\n\n--- Training Models for Handball ---\nClass distribution in y_train for Handball:\nmatch_result\n1    206\n0    123\nName: count, dtype: int64\nClass distribution in y_test for Handball:\nmatch_result\n1    47\n0    36\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 66.27%\nRandom Forest Accuracy: 60.24%\nLogistic Regression Accuracy: 56.63%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.65      0.47      0.55        36\n           1       0.67      0.81      0.73        47\n\n    accuracy                           0.66        83\n   macro avg       0.66      0.64      0.64        83\nweighted avg       0.66      0.66      0.65        83\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.57      0.36      0.44        36\n           1       0.62      0.79      0.69        47\n\n    accuracy                           0.60        83\n   macro avg       0.59      0.57      0.57        83\nweighted avg       0.59      0.60      0.58        83\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.50      0.08      0.14        36\n           1       0.57      0.94      0.71        47\n\n    accuracy                           0.57        83\n   macro avg       0.54      0.51      0.43        83\nweighted avg       0.54      0.57      0.46        83\n\n\n--- Training Models for Ice Hockey ---\nClass distribution in y_train for Ice Hockey:\nmatch_result\n1    430\n0    170\nName: count, dtype: int64\nClass distribution in y_test for Ice Hockey:\nmatch_result\n1    104\n0     46\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 72.00%\nRandom Forest Accuracy: 69.33%\nLogistic Regression Accuracy: 69.33%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.56      0.43      0.49        46\n           1       0.77      0.85      0.81       104\n\n    accuracy                           0.72       150\n   macro avg       0.66      0.64      0.65       150\nweighted avg       0.71      0.72      0.71       150\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.50      0.37      0.42        46\n           1       0.75      0.84      0.79       104\n\n    accuracy                           0.69       150\n   macro avg       0.62      0.60      0.61       150\nweighted avg       0.67      0.69      0.68       150\n\nLogistic Regression Classification Report:\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        46\n           1       0.69      1.00      0.82       104\n\n    accuracy                           0.69       150\n   macro avg       0.35      0.50      0.41       150\nweighted avg       0.48      0.69      0.57       150\n\n\n--- Training Models for Lacrosse ---\nClass distribution in y_train for Lacrosse:\nmatch_result\n1    182\n0     62\nName: count, dtype: int64\nClass distribution in y_test for Lacrosse:\nmatch_result\n1    44\n0    18\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 74.19%\nRandom Forest Accuracy: 77.42%\nLogistic Regression Accuracy: 70.97%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.11      0.20        18\n           1       0.73      1.00      0.85        44\n\n    accuracy                           0.74        62\n   macro avg       0.87      0.56      0.52        62\nweighted avg       0.81      0.74      0.66        62\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.62      0.56      0.59        18\n           1       0.83      0.86      0.84        44\n\n    accuracy                           0.77        62\n   macro avg       0.73      0.71      0.72        62\nweighted avg       0.77      0.77      0.77        62\n\nLogistic Regression Classification Report:\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        18\n           1       0.71      1.00      0.83        44\n\n    accuracy                           0.71        62\n   macro avg       0.35      0.50      0.42        62\nweighted avg       0.50      0.71      0.59        62\n\n\n--- Training Models for Roller Hockey ---\nClass distribution in y_train for Roller Hockey:\nmatch_result\n1    122\n0     83\nName: count, dtype: int64\nClass distribution in y_test for Roller Hockey:\nmatch_result\n0    27\n1    25\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 76.92%\nRandom Forest Accuracy: 59.62%\nLogistic Regression Accuracy: 48.08%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.76      0.81      0.79        27\n           1       0.78      0.72      0.75        25\n\n    accuracy                           0.77        52\n   macro avg       0.77      0.77      0.77        52\nweighted avg       0.77      0.77      0.77        52\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.59      0.74      0.66        27\n           1       0.61      0.44      0.51        25\n\n    accuracy                           0.60        52\n   macro avg       0.60      0.59      0.58        52\nweighted avg       0.60      0.60      0.59        52\n\nLogistic Regression Classification Report:\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        27\n           1       0.48      1.00      0.65        25\n\n    accuracy                           0.48        52\n   macro avg       0.24      0.50      0.32        52\nweighted avg       0.23      0.48      0.31        52\n\n\n--- Training Models for Rugby ---\nClass distribution in y_train for Rugby:\nmatch_result\n1    189\n0     71\nName: count, dtype: int64\nClass distribution in y_test for Rugby:\nmatch_result\n1    49\n0    16\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 76.92%\nRandom Forest Accuracy: 73.85%\nLogistic Regression Accuracy: 75.38%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.53      0.56      0.55        16\n           1       0.85      0.84      0.85        49\n\n    accuracy                           0.77        65\n   macro avg       0.69      0.70      0.70        65\nweighted avg       0.77      0.77      0.77        65\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.45      0.31      0.37        16\n           1       0.80      0.88      0.83        49\n\n    accuracy                           0.74        65\n   macro avg       0.63      0.60      0.60        65\nweighted avg       0.71      0.74      0.72        65\n\nLogistic Regression Classification Report:\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        16\n           1       0.75      1.00      0.86        49\n\n    accuracy                           0.75        65\n   macro avg       0.38      0.50      0.43        65\nweighted avg       0.57      0.75      0.65        65\n\n\n--- Training Models for Soccer ---\nClass distribution in y_train for Soccer:\nmatch_result\n1    402\n0    279\nName: count, dtype: int64\nClass distribution in y_test for Soccer:\nmatch_result\n1    101\n0     70\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 60.23%\nRandom Forest Accuracy: 57.89%\nLogistic Regression Accuracy: 57.31%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.57      0.11      0.19        70\n           1       0.61      0.94      0.74       101\n\n    accuracy                           0.60       171\n   macro avg       0.59      0.53      0.46       171\nweighted avg       0.59      0.60      0.51       171\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.49      0.49      0.49        70\n           1       0.64      0.64      0.64       101\n\n    accuracy                           0.58       171\n   macro avg       0.56      0.56      0.56       171\nweighted avg       0.58      0.58      0.58       171\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.36      0.06      0.10        70\n           1       0.59      0.93      0.72       101\n\n    accuracy                           0.57       171\n   macro avg       0.48      0.49      0.41       171\nweighted avg       0.50      0.57      0.47       171\n\n\n--- Training Models for Volleyball ---\nClass distribution in y_train for Volleyball:\nmatch_result\n1    459\n0    105\nName: count, dtype: int64\nClass distribution in y_test for Volleyball:\nmatch_result\n1    119\n0     23\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 87.32%\nRandom Forest Accuracy: 80.99%\nLogistic Regression Accuracy: 83.80%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.65      0.48      0.55        23\n           1       0.90      0.95      0.93       119\n\n    accuracy                           0.87       142\n   macro avg       0.78      0.71      0.74       142\nweighted avg       0.86      0.87      0.87       142\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.41      0.39      0.40        23\n           1       0.88      0.89      0.89       119\n\n    accuracy                           0.81       142\n   macro avg       0.65      0.64      0.64       142\nweighted avg       0.81      0.81      0.81       142\n\nLogistic Regression Classification Report:\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        23\n           1       0.84      1.00      0.91       119\n\n    accuracy                           0.84       142\n   macro avg       0.42      0.50      0.46       142\nweighted avg       0.70      0.84      0.76       142\n\n\n--- Training Models for Water Polo ---\nClass distribution in y_train for Water Polo:\nmatch_result\n0    113\n1     99\nName: count, dtype: int64\nClass distribution in y_test for Water Polo:\nmatch_result\n0    37\n1    17\nName: count, dtype: int64\nFitting 3 folds for each of 54 candidates, totalling 162 fits\nXGBoost Accuracy: 53.70%\nRandom Forest Accuracy: 55.56%\nLogistic Regression Accuracy: 48.15%\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.73      0.51      0.60        37\n           1       0.36      0.59      0.44        17\n\n    accuracy                           0.54        54\n   macro avg       0.54      0.55      0.52        54\nweighted avg       0.61      0.54      0.55        54\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.71      0.59      0.65        37\n           1       0.35      0.47      0.40        17\n\n    accuracy                           0.56        54\n   macro avg       0.53      0.53      0.52        54\nweighted avg       0.60      0.56      0.57        54\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.66      0.51      0.58        37\n           1       0.28      0.41      0.33        17\n\n    accuracy                           0.48        54\n   macro avg       0.47      0.46      0.45        54\nweighted avg       0.54      0.48      0.50        54\n\n"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "## Model Comparison Across All Sports\n\nFinally, we compare the accuracy of XGBoost, Random Forest, and Logistic Regression for each sport and print a summary.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Displaying results comparison for all sports\nprint(\"\\n--- Comparison of Model Results Across Sports ---\")\nfor sport, results in all_sports_results.items():\n    if results is not None:  # Ensure that results are not None\n        print(f\"\\nResults for {sport}:\")\n        for model, accuracy in results.items():\n            print(f\"{model}: {accuracy * 100:.2f}%\")\n    else:\n        print(f\"\\nSkipping {sport} due to insufficient class variety.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n--- Comparison of Model Results Across Sports ---\n\nResults for Basketball:\nXGBoost: 73.94%\nRandom Forest: 72.54%\nLogistic Regression: 58.45%\n\nResults for Cricket:\nXGBoost: 51.58%\nRandom Forest: 48.42%\nLogistic Regression: 64.21%\n\nResults for Hockey:\nXGBoost: 72.87%\nRandom Forest: 58.14%\nLogistic Regression: 61.24%\n\nResults for Handball:\nXGBoost: 66.27%\nRandom Forest: 60.24%\nLogistic Regression: 56.63%\n\nResults for Ice Hockey:\nXGBoost: 72.00%\nRandom Forest: 69.33%\nLogistic Regression: 69.33%\n\nResults for Lacrosse:\nXGBoost: 74.19%\nRandom Forest: 77.42%\nLogistic Regression: 70.97%\n\nResults for Roller Hockey:\nXGBoost: 76.92%\nRandom Forest: 59.62%\nLogistic Regression: 48.08%\n\nResults for Rugby:\nXGBoost: 76.92%\nRandom Forest: 73.85%\nLogistic Regression: 75.38%\n\nResults for Soccer:\nXGBoost: 60.23%\nRandom Forest: 57.89%\nLogistic Regression: 57.31%\n\nResults for Volleyball:\nXGBoost: 87.32%\nRandom Forest: 80.99%\nLogistic Regression: 83.80%\n\nResults for Water Polo:\nXGBoost: 53.70%\nRandom Forest: 55.56%\nLogistic Regression: 48.15%\n"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}